{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWw2cPzcovZz"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "**Due to**: 9th November, 2020\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Word embeddings, from sparse to dense representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xgH0v9hdTKa"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this assignment we will explore text encoding techniques, spanning from sparse representations, such as bag-of-words, to dense representations.\n",
        "\n",
        "In particular, we will see:\n",
        "\n",
        "*   Building a vocabulary\n",
        "*   Building a word-word co-occurrence matrix\n",
        "*   Defining a similarity metric: cosine similarity\n",
        "*   Embedding visualization and analysis of their semantic properties\n",
        "*   Better sparse representations via PPMI weighting\n",
        "*   Loading pre-trained dense word embeddings (Word2Vec, GloVe)\n",
        "*   Checking out-of-vocabulary (OOV) terms\n",
        "*   Handling OOV terms\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUQAeC3gqKhJ"
      },
      "source": [
        "# Initial Setup\n",
        "\n",
        "First of all, we need to import some useful packages that we will use during this hands-on session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_zjgza4qZYE"
      },
      "source": [
        "# system packages\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "# data and numerical management packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# useful during debugging (progress bars)\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI_8CixDqhcm"
      },
      "source": [
        "# [Part I] Sparse embeddings\n",
        "\n",
        "As you know, working with text inherently requires a conversion step, formally known as embedding, that simply allows us to pass from string-like text to corresponding numerical representation. \n",
        "\n",
        "One of the most notable embedding method is the bag-of-words one (BoW). We simply count the occurrence of each word in the given corpus so as to build some useful data structures (matrices) that may give us some general idea of how the dataset is organized. For example, we can check where a particular word appears or, in a reversed perspective, identify the most common terms in each given document.\n",
        "\n",
        "This type of reasoning is directly related to how meaning is assigned to words. In particular, it is the environment itself, enclosing a word, that gives a specific meaning to it. Thus, we look for numerical encoding methods that reflect such point of view.\n",
        "\n",
        "Before diving into embedding analysis, we need to prepare a dataset and, most importantly, extract a vocabulary!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MmTNaGpv6L_"
      },
      "source": [
        "## Prepare a dataset for experiments\n",
        "\n",
        "We will use the IMDB dataset of previous assignment. As you already know, it is a dataset of 50k sentences used for sentiment analysis. In particular, half of them (25k) is labelled as containing positive sentiment, whereas the remaining half are sentences of negative polarity.\n",
        "\n",
        "Contrarily to first assignment, we will ignore sentiment labels and we will focus only on learning a proper word embedding representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWWAMPeTzfSh"
      },
      "source": [
        "### Download and extraction\n",
        "\n",
        "We start by downloading the dataset and extract it to a folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VphzaMCxZps"
      },
      "source": [
        "from urllib import request\n",
        "import tarfile\n",
        "\n",
        "# Config\n",
        "print(\"Current work directory: {}\".format(os.getcwd()))\n",
        "\n",
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"Movies.tar.gz\")\n",
        "\n",
        "print(dataset_path)\n",
        "\n",
        "def download_dataset(download_path, url):\n",
        "    if not os.path.exists(download_path):\n",
        "        print(\"Downloading dataset...\")\n",
        "        request.urlretrieve(url, download_path)\n",
        "        print(\"Download complete!\")\n",
        "\n",
        "def extract_dataset(download_path, extract_path):\n",
        "    print(\"Extracting dataset... (it may take a while...)\")\n",
        "    with tarfile.open(download_path) as loaded_tar:\n",
        "        loaded_tar.extractall(extract_path)\n",
        "    print(\"Extraction completed!\")\n",
        "\n",
        "# Download\n",
        "download_dataset(dataset_path, url)\n",
        "\n",
        "# Extraction\n",
        "extract_dataset(dataset_path, dataset_folder)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Naaatxml3idO"
      },
      "source": [
        "Feel free to check the dataset folder content. Usually, the README file is a good starting point (if it exists and its well done, which is not so common!).\n",
        "\n",
        "Just like in the first assignment, we need a high level view of the dataset that is helpful to our needs. Thus, we will encode the dataset into a [pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvPbnTfOSvbT"
      },
      "source": [
        "\n",
        "# Config\n",
        "dataset_name = \"aclImdb\"\n",
        "debug = True\n",
        "\n",
        "def encode_dataset(dataset_name, debug=True):\n",
        "    dataframe_rows = []\n",
        "\n",
        "    for split in tqdm(['train', 'test']):\n",
        "        for sentiment in ['pos', 'neg']:\n",
        "            folder = os.path.join(os.getcwd(), \"Datasets\", dataset_name, split, sentiment)\n",
        "            for filename in os.listdir(folder):\n",
        "                file_path = os.path.join(folder, filename)\n",
        "                try:\n",
        "                    if os.path.isfile(file_path):\n",
        "                        # open the file\n",
        "                        with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "                            # read it and extract informations\n",
        "                            text = text_file.read()\n",
        "                            score = filename.split(\"_\")[1].split(\".\")[0]\n",
        "                            file_id = filename.split(\"_\")[0]\n",
        "\n",
        "                            num_sentiment = -1\n",
        "\n",
        "                            if sentiment == \"pos\" : num_sentiment = 1\n",
        "                            elif sentiment == \"neg\" : num_sentiment = 0\n",
        "\n",
        "                            # create single dataframe row\n",
        "                            dataframe_row = {\n",
        "                                \"file_id\": file_id,\n",
        "                                \"score\": score,\n",
        "                                \"sentiment\": num_sentiment,\n",
        "                                \"split\": split,\n",
        "                                \"text\": text\n",
        "                            }\n",
        "\n",
        "                            # print detailed info for the first file\n",
        "                            if debug:\n",
        "                                print(file_path)\n",
        "                                print(filename)\n",
        "                                print(file_id)\n",
        "                                print(text)\n",
        "                                print(score)\n",
        "                                print(sentiment)\n",
        "                                print(split)\n",
        "                                print(dataframe_row)\n",
        "                                debug = False\n",
        "                            dataframe_rows.append(dataframe_row)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print('Failed to process %s. Reason: %s' % (file_path, e))\n",
        "                    sys.exit(0)\n",
        "\n",
        "    folder = os.path.join(os.getcwd(), \"Datasets\", \"Dataframes\", dataset_name)\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    # transform the list of rows in a proper dataframe\n",
        "    df = pd.DataFrame(dataframe_rows)\n",
        "    df = df[[\"file_id\",\n",
        "                        \"score\",\n",
        "                        \"sentiment\",\n",
        "                        \"split\",\n",
        "                        \"text\"]]\n",
        "    dataframe_path = os.path.join(folder, dataset_name + \".pkl\")\n",
        "    df.to_pickle(dataframe_path)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Encoding\n",
        "print(\"Encoding dataset...\")\n",
        "df = encode_dataset(dataset_name, debug)\n",
        "print(\"Encoding completed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHCroMaDdQdB"
      },
      "source": [
        "### Loading and Visualization\n",
        "\n",
        "The next step is to load the dataset and inspect some of its elements in order to have an idea of the general content. We will use **pandas** library for dataset loading as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9itQUTUA3e_C"
      },
      "source": [
        "# Inspection\n",
        "\n",
        "print(\"Dataset size: {}\".format(df.shape)) # (50000, 5)\n",
        "print(\"Dataset columns: {}\".format(df.columns.values)) # ['file_id', 'score', 'sentiment', 'split', 'text]\n",
        "\n",
        "print(\"Classes distribution:\\n{}\".format(df.sentiment.value_counts())) # [0: 25000, 1: 25000]\n",
        "\n",
        "print(\"Some examples: {}\".format(df.iloc[:5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuuACGZs5fLS"
      },
      "source": [
        "Feel free to inspect the dataset as you wish in the following code space!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTYo05YVeu7e"
      },
      "source": [
        "### [Optional] A quick simplification\n",
        "\n",
        "Since the dataset is quite large, the embedding related methods, such as co-occurrence matrix construction, may take a while or may require ad hoc solutions. For instance, if we consider the whole dataset (50k sentences) the vocabulary should be around 160k terms and we don't have sufficient memory to load a (160k, 160k) co-occurrence matrix.\n",
        "\n",
        "For the purpose of this assignment, we can rely on a small slice of the dataset.\n",
        "In this way, we can get results in small amount of time. Nonetheless, feel free\n",
        "to work with the whole dataset! Suggestions on how to handle this scenario are\n",
        "given below when required.\n",
        "\n",
        "Select the amount of dataset samples you want to keep and re-define the dataset as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyghmotufcFe"
      },
      "source": [
        "samples_amount = 10000\n",
        "\n",
        "# This type of slicing is not mandatory,\n",
        "# but it is sufficient to our purposes\n",
        "np.random.seed(42)\n",
        "random_indexes = np.random.choice(np.arange(df.shape[0]),\n",
        "                                  size=samples_amount,\n",
        "                                  replace=False)\n",
        "\n",
        "df = df.iloc[random_indexes]\n",
        "\n",
        "print('New dataset size: ', df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohww_Fda5wR-"
      },
      "source": [
        "## Building the Vocabulary\n",
        "\n",
        "At this point we can build the word vocabulary of our dataset. This information is the first step of any word embedding method: we need to know the set of atomic entities that build up our corpus.\n",
        "\n",
        "**Definition**: a vocabulary is a collection of words occurring in a given dataset. More precisely, each word is recognized and assigned an index.\n",
        "\n",
        "**Example**: Suppose you have the given toy corpus $D$: { \"the cat is on the table\" }\n",
        "\n",
        "As you notice, the dataset is comprised of only one sentence: \"the cat is on the table\". The corresponding vocabulary (a possible one) will be:\n",
        "\n",
        "V = {0: 'the', 1: 'cat', 2: 'is', 3: 'on', 4: 'table'}\n",
        "\n",
        "In this case, indexing follows word order, but it is not mandatory!\n",
        "\n",
        "**Important**: The most important thing to remember is that the vocabulary should always be the same one. Thus, make sure that the vocabulary creation routine always returns the same result!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYOu4FV067O_"
      },
      "source": [
        "### Some Cleaning\n",
        "\n",
        "Before vocabulary creation, we have to do a little bit of text pre-processing so as to avoid spurious data.\n",
        "\n",
        "Pre-processing is always an important step in any machine learning based task, since data quality is one of the crucial factors that lead to better performance. Models, even state-of-the-art ones, hardly achieve satisfying results if the dataset is very noisy.\n",
        "\n",
        "**Types of pre-processing**: there are a lot of pre-processing steps that we can consider, either general or quite task- specific. Here we will rely on very standard and simple methods.\n",
        "\n",
        "*    **Text to lower**: casing usually doesn't affect our task, but in some scenarios, such as part-of-speech tagging, might even be crucial.\n",
        "\n",
        "*    **Replace special characters**: special characters are usually employed as variants of a single character like the spacing symbol ' '. \n",
        "In other cases (dates, etc..) special characters might have a specific meaning and should not be replaced.\n",
        "\n",
        "*    **Text stripping**: it is important to filter out extra spaces to avoid unwanted distinctions between identical words, such as 'apple' and ' apple '.\n",
        "\n",
        "There are a lot of pre-processing techniques, such as number replacing, lemmatization, stemming, spell correction, acronyms merge and so on. If you are interested you can check [here](https://medium.com/swlh/text-normalization-7ecc8e084e31) and [here](https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html) some good blogs about the topic.\n",
        "\n",
        "**NOTE**: If you feel like there should be some additional pre-processing, feel free to modify this section as you please! Please, remember to provide additional comments to motivate your changes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TLTu0-2JQwi"
      },
      "source": [
        "import re\n",
        "from functools import reduce\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Config\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "GOOD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "try:\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "    \n",
        "STOPWORDS.add(\"br\") #nicely handles br HTML tag\n",
        "\n",
        "def lower(text):\n",
        "    \"\"\"\n",
        "    Transforms given text to lower case.\n",
        "    Example:\n",
        "    Input: 'I really like New York city'\n",
        "    Output: 'i really like new your city'\n",
        "    \"\"\"\n",
        "\n",
        "    return text.lower()\n",
        "\n",
        "def replace_special_characters(text):\n",
        "    \"\"\"\n",
        "    Replaces special characters, such as paranthesis,\n",
        "    with spacing character\n",
        "    \"\"\"\n",
        "\n",
        "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "\n",
        "def filter_out_uncommon_symbols(text):\n",
        "    \"\"\"\n",
        "    Removes any special character that is not in the\n",
        "    good symbols list (check regular expression)\n",
        "    \"\"\"\n",
        "    \n",
        "    # changed '' to ' ': separates <br/> from the end of words: filmbr \n",
        "    # and splits hypen-compound words instead of aggregating them into a single one.\n",
        "    return GOOD_SYMBOLS_RE.sub(' ', text)\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
        "\n",
        "\n",
        "def strip_text(text):\n",
        "    \"\"\"\n",
        "    Removes any left or right spacing (including carriage return) from text.\n",
        "    Example:\n",
        "    Input: '  This assignment is cool\\n'\n",
        "    Output: 'This assignment is cool'\n",
        "    \"\"\"\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "PREPROCESSING_PIPELINE = [\n",
        "                          lower,\n",
        "                          replace_special_characters,\n",
        "                          filter_out_uncommon_symbols,\n",
        "                          remove_stopwords,\n",
        "                          strip_text\n",
        "                          ]\n",
        "\n",
        "# Anchor method\n",
        "\n",
        "def text_prepare(text, filter_methods=None):\n",
        "    \"\"\"\n",
        "    Applies a list of pre-processing functions in sequence (reduce).\n",
        "    Note that the order is important here!\n",
        "    \"\"\"\n",
        "\n",
        "    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n",
        "\n",
        "    return reduce(lambda txt, f: f(txt), filter_methods, text)\n",
        "\n",
        "# Pre-processing\n",
        "\n",
        "print('Pre-processing text...')\n",
        "\n",
        "print()\n",
        "print('[Debug] Before:\\n{}'.format(df.text[:3]))\n",
        "print()\n",
        "\n",
        "# Replace each sentence with its pre-processed version\n",
        "df['text'] = df['text'].apply(lambda txt: text_prepare(txt))\n",
        "\n",
        "print('[Debug] After:\\n{}'.format(df.text[:3]))\n",
        "print()\n",
        "\n",
        "#</br> removal\n",
        "br_test = 'He wins the game! <br>'\n",
        "print(f\"{br_test} -> {text_prepare(br_test)}\")\n",
        "print()\n",
        "\n",
        "print(\"Pre-processing completed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YphBz_iaMRm0"
      },
      "source": [
        "### **Vocabulary Creation**\n",
        "\n",
        "We are now ready to create the vocabulary! This task is up to you! Complete the below function and remember to follow mentioned requirements.\n",
        "\n",
        "FYI, since the text has been pre-processed, space splitting should work correctly. \n",
        "\n",
        "Bare in mind that some packages offers tools for automatic vocabulary creation, such as Keras (check [keras.preprocessing.text.Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)). \n",
        "\n",
        "**NOTE**: It is not mandatory to use the keras Tokenizer, we mention it so that you know there exist tools specific for this step.\n",
        "\n",
        "**NOTE**: In this case, the vocabulary will start from index equal to 1, since the 0 slot is reserved to padding token. In order to pass vocabulary evaluation, you have to re-scale the vocabulary such that the first index is 0.\n",
        "\n",
        "**NOTE**: If you are using the keras Tokenizer, remember to use its method <code> texts_to_sequences </code> for splitting, otherwise you might find terms that are not in the vocabulary! Please, check also its constructor argument <code> filters </code> since it defines a pre-processing regexp.\n",
        "\n",
        "This easy task is just to let you know that is important to check the built vocabulary just to make sure everything is ok."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRs7m-SxMq0n"
      },
      "source": [
        "from collections import OrderedDict\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Function definition\n",
        "def build_vocabulary(df,text_prepare=text_prepare):\n",
        "    \"\"\"\n",
        "    Given a dataset, builds the corresponding word vocabulary.\n",
        "\n",
        "    :param df: dataset from which we want to build the word vocabulary (pandas.DataFrame)\n",
        "    :param text_prepare: text pre-processing function\n",
        "    \n",
        "    :return:\n",
        "      - word vocabulary: vocabulary index to word\n",
        "      - inverse word vocabulary: word to vocabulary index\n",
        "      - word listing: set of unique terms that build up the vocabulary\n",
        "    \"\"\"\n",
        "    unique_tokens = set(nltk.word_tokenize(' '.join([text_prepare(doc) for doc in df['text']])))\n",
        "    voc = OrderedDict() #{0: PAD}\n",
        "    rev_voc = OrderedDict() #{PAD: 0}\n",
        "    for i, token in enumerate(unique_tokens):\n",
        "        voc[i] = token\n",
        "        rev_voc[token] = i\n",
        "    #unique_tokens.add(PAD)\n",
        "    return voc, rev_voc, unique_tokens\n",
        "\n",
        "\n",
        "# Testing\n",
        "idx_to_word, word_to_idx, word_listing = build_vocabulary(df)\n",
        "\n",
        "print('[Debug] Index -> Word vocabulary size: {}'.format(len(idx_to_word)))\n",
        "print('[Debug] Word -> Index vocabulary size: {}'.format(len(word_to_idx)))\n",
        "\n",
        "print('[Debug] Some words: {}'.format([(idx_to_word[idx], idx) for idx in np.arange(10) + 1]))\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "def evaluate_vocabulary(idx_to_word, word_to_idx, word_listing, df, check_default_size=False):\n",
        "\n",
        "    # Check size\n",
        "    print(\"[Vocabulary Evaluation] Size checking...\")\n",
        "\n",
        "    assert len(idx_to_word) == len(word_to_idx)\n",
        "    assert len(idx_to_word) == len(word_listing)\n",
        "\n",
        "    # Check content\n",
        "    print(\"[Vocabulary Evaluation] Content checking...\")\n",
        "\n",
        "    for i in tqdm(range(0, len(idx_to_word))):\n",
        "        assert idx_to_word[i] in word_to_idx\n",
        "        assert word_to_idx[idx_to_word[i]] == i\n",
        "\n",
        "    # Check consistency\n",
        "    print(\"[Vocabulary Evaluation] Consistency checking...\")\n",
        "\n",
        "    _, _, first_word_listing = build_vocabulary(df)\n",
        "    _, _, second_word_listing = build_vocabulary(df)\n",
        "    assert first_word_listing == second_word_listing\n",
        "\n",
        "    # Check toy example\n",
        "    print(\"[Vocabulary Evaluation] Toy example checking...\")\n",
        "    toy_df = pd.DataFrame.from_dict({\n",
        "        'text': [\"all that glitters is not gold\", \"all in all i like this assignment\"]\n",
        "    })\n",
        "    _, _, toy_word_listing = build_vocabulary(toy_df,text_prepare=lambda x:x) #suppresses text_prepare for the toy test\n",
        "    toy_valid_vocabulary = set(' '.join(toy_df.text.values).split())\n",
        "    print(toy_word_listing)\n",
        "    print(toy_valid_vocabulary)\n",
        "    assert set(toy_word_listing) == toy_valid_vocabulary\n",
        "\n",
        "\n",
        "print(\"Vocabulary evaluation...\")\n",
        "evaluate_vocabulary(idx_to_word, word_to_idx, word_listing, df)\n",
        "print(\"Evaluation completed!\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pUTYnmcFQct"
      },
      "source": [
        "### **Save the vocabulary**\n",
        "\n",
        "Generally speaking, it is a good idea to save the dictionary in clear format. In this way you can quickly check for errors or useful words.\n",
        "\n",
        "In this case, we will save the vocabulary dictionary in JSON format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg41avgpFlFJ"
      },
      "source": [
        "!pip install simplejson\n",
        "import simplejson as sj\n",
        "\n",
        "vocab_path = os.path.join(os.getcwd(), 'Datasets', dataset_name, 'vocab.json')\n",
        "\n",
        "print(\"Saving vocabulary to {}\".format(vocab_path))\n",
        "with open(vocab_path, mode='w') as f:\n",
        "    sj.dump(word_to_idx, f, indent=4)\n",
        "print(\"Saving completed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrWb80lyY629"
      },
      "source": [
        "## Building the Co-occurence Matrix\n",
        "\n",
        "As we said at the beginning, embedding methods are based on the principle that similar words will be used in similar contexts. Thus, context information is crucial to determine the meaning of a word.\n",
        "\n",
        "One basic approach, which falls under the category of sparse representations, is the **co-occurrence matrix**: for each word in the vocabulary we count the number of times each other word appears within the same context window. A simple example is given by image below.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1UknGoYvIBBA7ytkSlqm1NhF_lHt0iOwT)\n",
        "\n",
        "In particular, the context window defines our notion of word context. Consider the following example:\n",
        "\n",
        "<h3><center> The cat is on the table </center></h3>\n",
        "\n",
        "We have to consider each word in the sentence and for each we have count words within the context window. Suppose a window of size 2, then we have for the word 'cat':\n",
        "\n",
        "Current word: cat\n",
        "\n",
        "Context words: [the, is, on]\n",
        "\n",
        "Notice how we consider $W$ words back and ahead of current word, where $W$ is the window size.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Let's define the simplest version of a **co-occurrence matrix** based on word counting.\n",
        "\n",
        "---\n",
        "\n",
        "**Small dataset case**: If you selected a small slice of the dataset, you should have a vocabulary size that we can afford in terms of memory demand. Thus, you can easily instantiate the co-occurrence matrix and populate it iteratively.\n",
        "\n",
        "---\n",
        "\n",
        "**Large dataset case**: We have to work with sparse matrices due to the high vocabulary size and to the low amount of non-zero word counts. To this end, the [Scipy package](https://docs.scipy.org/doc/scipy/reference/sparse.html) allows us to easily define sparse matrices that can be converted ot numpy arrays (if we can).\n",
        "\n",
        "**Suggestion**: The simplest way to build the co-occurrence matrix is via an incremental approach: we loop through dataset sentences, split into words and then count co-occurrences within the given window frame. Generally, combining this approach with sparse matrices is not so efficient (yet possible). However, Scipy offers [$\\texttt{lil_matrix}$](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html#scipy.sparse.lil_matrix) sparse format that is suitable to this case. Anyway, check out other sparse formats, such as [$\\texttt{csr_matrix}$](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix), and the corresponding building methods.\n",
        "\n",
        "Working with $\\texttt{lil_matrix}$ might take $\\sim 1h$ of time to build the whole dataset co-occurrence matrix. It is also possibile to work with $\\texttt{csr_matrix}$ but the approach is more complex (check the last example of the corresponding documentation page).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS5H65sh52b9"
      },
      "source": [
        "import scipy    # defines several types of efficient sparse matrices\n",
        "import zipfile\n",
        "import gc\n",
        "import requests\n",
        "import time\n",
        "\n",
        "### CHOSE SPARSITY FOR CO_OCCURRENCE_MATRIX\n",
        "SPARSE = True\n",
        "\n",
        "# Function definition\n",
        "\n",
        "def co_occurrence_count(df, idx_to_word, word_to_idx, window_size=3, text_prepare=text_prepare, sparse=True):\n",
        "    \"\"\"\n",
        "    Builds word-word co-occurrence matrix based on word counts.\n",
        "\n",
        "    :param df: pre-processed dataset (pandas.DataFrame)\n",
        "    :param idx_to_word: vocabulary map (index -> word) (dict)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param text_prepare: text pre-processing function\n",
        "\n",
        "    :return\n",
        "      - co_occurrence symmetric matrix of size |V| x |V| (|V| = vocabulary size)\n",
        "    \"\"\"\n",
        "    vocab_count = len(idx_to_word)\n",
        "    if sparse:\n",
        "        #lil_matrix is designed for building sparse matrices fast\n",
        "        co_occurrence_matrix = scipy.sparse.lil_matrix((len(idx_to_word), len(idx_to_word)), dtype=int)\n",
        "    else:\n",
        "        co_occurrence_matrix = np.zeros(shape=(vocab_count, vocab_count), dtype='float32')\n",
        "    \n",
        "    for doc in tqdm(df[\"text\"]):\n",
        "        tokenized = nltk.word_tokenize(text_prepare(doc))\n",
        "        for i, token in enumerate(tokenized):\n",
        "            \n",
        "            # words inside the window are selected by slicing carefully:\n",
        "            # max handles the start-of-sentence  [_ _ IS on the] table\n",
        "            # window_size+1 allows for correct indexing\n",
        "            window = tokenized[max(i-window_size, 0) : i+window_size+1]\n",
        "            #i=0, window_size = 4: 0-4:0+4+1 -> [0:5]\n",
        "            #i=5, window_size = 4: 5-4:5+4+1 -> [1:10]\n",
        "            target_id = word_to_idx[token]\n",
        "            context_ids = [word_to_idx[t] for t in window]\n",
        "            \n",
        "            for dd in context_ids:\n",
        "                # we checked if we could reduce the dtype size to uint16, but it overflows\n",
        "                # assert co_occurrence_matrix[target_id, dd] < 65535\n",
        "                co_occurrence_matrix[target_id, dd] += 1\n",
        "                \n",
        "    co_occurrence_matrix[np.diag_indices(vocab_count)] = 0 # zeroes diag\n",
        "    \n",
        "    if sparse:\n",
        "        #csr_matrix/csc are designed for storing and computing with sparse matrices fast\n",
        "        return scipy.sparse.csr_matrix(co_occurrence_matrix)\n",
        "    return co_occurrence_matrix\n",
        "\n",
        "# Testing\n",
        "window_size = 3\n",
        "\n",
        "# Clean RAM before re-running this code snippet to avoid session crash\n",
        "if 'co_occurrence_matrix' in globals():\n",
        "    del co_occurrence_matrix\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "if SPARSE:\n",
        "    print(\"\\nBuilding sparse co-occurrence count matrix... (it may take a while...)\")\n",
        "else:\n",
        "    print(\"\\nBuilding dense co-occurrence count matrix... (it may take a while...)\")\n",
        "\n",
        "start = time.time()\n",
        "co_occurrence_matrix = co_occurrence_count(df, idx_to_word, word_to_idx, window_size,\n",
        "                                           sparse = SPARSE)\n",
        "time_dense = time.time() -start\n",
        "time_sparse = time_dense\n",
        "\n",
        "print(\"Building completed!\")\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "def download_toy_data(benchmark_path):\n",
        "    toy_data_path = os.path.join(benchmark_path, 'co-occurrence_count_benchmark.zip')\n",
        "    toy_data_url_id = \"1z8qp034utvW7kv-9Q_TACJv3_sdCzkZg\"\n",
        "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    if not os.path.exists(benchmark_path):\n",
        "        os.makedirs(benchmark_path)\n",
        "\n",
        "    if not os.path.exists(toy_data_path):\n",
        "        print(\"Downloading co-occurrence count matrix benchmark data...\")\n",
        "        with requests.Session() as current_session:\n",
        "            response = current_session.get(toy_url,\n",
        "                                   params={'id': toy_data_url_id},\n",
        "                                   stream=True)\n",
        "        save_response_content(response, toy_data_path)\n",
        "        print(\"Download complete!\")\n",
        "\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
        "            loaded_zip.extractall(benchmark_path)\n",
        "        print(\"Extraction complete!\")\n",
        "\n",
        "def evaluate_co_occurrence_matrix(matrix):\n",
        "    is_sparse = False\n",
        "\n",
        "    if hasattr(scipy.sparse, type(matrix).__name__):\n",
        "        print(\"Detected sparse co-occurrence matrix!\")\n",
        "        is_sparse = True\n",
        "\n",
        "    # Check symmetry\n",
        "    print(\"[Co-occurrence count matrix Evaluation] Symmetry checking...\")\n",
        "    if is_sparse:\n",
        "        assert (matrix != matrix.transpose()).nnz == 0\n",
        "    else:\n",
        "        assert np.equal(matrix, matrix.transpose()).all()\n",
        "\n",
        "    # Check toy example\n",
        "    print(\"[Co-occurrence count matrix Evaluation] Toy example checking...\")\n",
        "    toy_df = pd.DataFrame.from_dict({\n",
        "        'text': [\"all that glitters is not gold\",\n",
        "                 \"all in all i like this assignment\"],\n",
        "    })\n",
        "    benchmark_path = os.path.join(os.getcwd(), 'Benchmark')\n",
        "    toy_path = os.path.join(benchmark_path, 'co-occurrence_count_benchmark')\n",
        "    download_toy_data(benchmark_path)\n",
        "\n",
        "    toy_idx_to_word = np.load(os.path.join(toy_path, 'toy_idx_to_word.npy'), allow_pickle=True).item()\n",
        "    toy_word_to_idx = np.load(os.path.join(toy_path, 'toy_word_to_idx.npy'), allow_pickle=True).item()\n",
        "\n",
        "    toy_matrix = co_occurrence_count(toy_df, toy_idx_to_word, toy_word_to_idx, window_size=1, text_prepare=lambda x:x)\n",
        "    toy_valid_matrix = np.load(os.path.join(toy_path, 'toy_co_occurrence_matrix_count.npy'))\n",
        "\n",
        "    if is_sparse:\n",
        "        assert np.equal(toy_matrix.todense(), toy_valid_matrix).all()\n",
        "    else:\n",
        "        assert np.equal(toy_matrix, toy_valid_matrix).all()\n",
        "\n",
        "\n",
        "print(\"Evaluating co-occurrence matrix\")\n",
        "print()\n",
        "#print(f\"Co-occurrence matrix takes up {co_occurrence_matrix.nbytes/1024/1024:.2f} MB\")\n",
        "\n",
        "evaluate_co_occurrence_matrix(co_occurrence_matrix)\n",
        "print(\"Evaluation completed!\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFPTM2RsGHAn"
      },
      "source": [
        "### Dense vs. Sparse Matrix Resource Usage Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL3CmYFTltXs"
      },
      "source": [
        "if SPARSE:\n",
        "    # dense already computed\n",
        "    print(\"Computing dense co-occurrence matrix...\")\n",
        "    sparse = co_occurrence_matrix\n",
        "\n",
        "    start = time.time()\n",
        "    dense = co_occurrence_count(df, idx_to_word, word_to_idx, window_size, sparse=False)\n",
        "    time_dense = time.time() - start\n",
        "    \n",
        "else:\n",
        "    # sparse already computed\n",
        "    print(\"Computing sparse co-occurrence matrix...\")\n",
        "    dense = co_occurrence_matrix\n",
        "\n",
        "    start = time.time()\n",
        "    sparse = co_occurrence_count(df, idx_to_word, word_to_idx, window_size, sparse=True)\n",
        "    time_sparse = time.time() - start\n",
        "\n",
        "print(\"\\n\\n\\t\\tDense\\tSparse\\tUsing Sparse Matrix:\\n\")\n",
        "print(f\"Memory in MB\\t{dense.nbytes/1024/1024:.0f}\\t{sparse.data.nbytes/1024/1024:.0f}\\tMemory reduces to {sparse.data.nbytes/ dense.nbytes:.2%}\")\n",
        "print(f\"Time in Seconds\\t{time_dense:.2f}\\t{time_sparse:.2f}\\tBuild time increases to {time_sparse/time_dense:.0%}\\n\")\n",
        "print(\"For \",len(idx_to_word),\" different words\")\n",
        "\n",
        "del dense\n",
        "del sparse\n",
        "gc.collect();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H10Eqj5-CyZv"
      },
      "source": [
        "## Embedding Visualization\n",
        "\n",
        "The next step is to visualize our sparse word embeddings in a lower dimensional space (2D) in order to have an idea of the meaning attributed to each word.\n",
        "\n",
        "**How?** Well, there are some dimensionality reduction techniques that we might employ. We will explore SVD and t-SNE methods, without delving into technical details since they are not arguments of this NLP course.\n",
        "\n",
        "**SVD Memo**: SVD stands for **Singular Value Decomposition** and is a kind of generalized **Principal Components Analysis** (PCA) and focuses on selecting the top **k** principal components. For more info, [here](https://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf) you can find a brief tutorial.\n",
        "\n",
        "**t-SNE Memo**: t-SNE stands for **t-Distributed Stochastic Neighbour Embedding** and is an unsupervised non-linear technique. The non-linearity is one major point of difference with PCA. Additionally, it preserves small pairwise distance (or local similarities), whereas PCA aims to preserve large pairwise distances in order to maximize variance. The basic idea of t-SNE is to compute a similarity measure between a pair of instances both at high and low dimensional space and optimize these two similarities via a cost function. Properly using t-SNE is a bit tricky, a well recommended reading is one of the [author's blog](https://lvdmaaten.github.io/tsne/).\n",
        "\n",
        "**Note**: We strongly suggest you to play with the window size and check if there are some notable differences. Generally, a small window size reflects syntactic properties, while a large window size captures semantic ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNzFd1-4JBvS"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function definition\n",
        "\n",
        "def visualize_embeddings(embeddings, word_annotations1=None, word_annotations2=None, word_to_idx=None):\n",
        "    \"\"\"\n",
        "    Plots given reduce word embeddings (2D).\n",
        "    Users can highlight specific words (word_annotations list) in order to better\n",
        "    analyse the effectiveness of the embedding method.\n",
        "\n",
        "    :param embeddings: word embedding matrix of shape (words, 2) retrieved via a\n",
        "                       dimensionality reduction technique.\n",
        "    :param word_annotations: list of words to be annotated.\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    \"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
        "\n",
        "    if word_annotations1:\n",
        "        print(\"Annotating words1: {}\".format(word_annotations1))\n",
        "\n",
        "        word_indexes = []\n",
        "        for word in word_annotations1:\n",
        "            word_index = word_to_idx[word]\n",
        "            word_indexes.append(word_index)\n",
        "\n",
        "        word_indexes = np.array(word_indexes)\n",
        "        target_embeddings = embeddings[word_indexes]\n",
        "        ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1.0, c='red')\n",
        "        ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1, facecolors='none', edgecolors='r', s=1000)\n",
        "\n",
        "\n",
        "        for word, word_index in zip(word_annotations1, word_indexes):\n",
        "            word_x, word_y = embeddings[word_index, 0], embeddings[word_index, 1]\n",
        "            ax.annotate(word, xy=(word_x, word_y))\n",
        "    else:\n",
        "        ax.scatter(embeddings[:, 0], embeddings[:, 1], alpha=0.1, c='blue')\n",
        "\n",
        "    if word_annotations2:\n",
        "        print(\"Annotating words2: {}\".format(word_annotations2))\n",
        "\n",
        "        word_indexes = []\n",
        "        for word in word_annotations2:\n",
        "            word_index = word_to_idx[word]\n",
        "            word_indexes.append(word_index)\n",
        "\n",
        "        word_indexes = np.array(word_indexes)\n",
        "        target_embeddings = embeddings[word_indexes]\n",
        "        ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1.0, c='yellow')\n",
        "        ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1, facecolors='none', edgecolors='y', s=1000)\n",
        "        \n",
        "        for word, word_index in zip(word_annotations2, word_indexes):\n",
        "            word_x, word_y = embeddings[word_index, 0], embeddings[word_index, 1]\n",
        "            ax.annotate(word, xy=(word_x, word_y))\n",
        "    else:\n",
        "        ax.scatter(embeddings[:, 0], embeddings[:, 1], alpha=0.1, c='blue')\n",
        "\n",
        "    other_embeddings = embeddings[np.setdiff1d(np.arange(embeddings.shape[0]), word_indexes)]\n",
        "    ax.scatter(other_embeddings[:, 0], other_embeddings[:, 1], alpha=0.1, c='blue')\n",
        "         \n",
        "    \n",
        "\n",
        "    # Set proper axis limit range\n",
        "    # We avoid outliers ruining the visualization if they are quite far away\n",
        "    xmin_quantile = np.quantile(embeddings[:, 0], q=0.01)\n",
        "    xmax_quantile = np.quantile(embeddings[:, 0], q=0.99)\n",
        "\n",
        "    ymin_quantile = np.quantile(embeddings[:, 1], q=0.01)\n",
        "    ymax_quantile = np.quantile(embeddings[:, 1], q=0.99)\n",
        "\n",
        "    ax.set_xlim(xmin_quantile, xmax_quantile)\n",
        "    ax.set_ylim(ymin_quantile, ymax_quantile)\n",
        "\n",
        "\n",
        "def reduce_SVD(embeddings):\n",
        "    \"\"\"\n",
        "    Applies SVD dimensionality reduction.\n",
        "\n",
        "    :param embeddings: word embedding matrix of shape (words, dim). In the case\n",
        "                       of a word-word co-occurrence matrix the matrix shape would\n",
        "                       be (words, words).\n",
        "\n",
        "    :return\n",
        "        - 2-dimensional word embedding matrix of shape (words, 2)\n",
        "    \"\"\"\n",
        "  \n",
        "    print(\"Running SVD reduction method...\")\n",
        "    svd = TruncatedSVD(n_components=2, n_iter=10, random_state=42)\n",
        "    reduced = svd.fit_transform(embeddings)\n",
        "    print(\"SVD reduction completed!\")\n",
        "\n",
        "    return reduced\n",
        "\n",
        "# Note: this method may take a while\n",
        "def reduce_tSNE(embeddings):\n",
        "    \"\"\"\n",
        "    Applies t-SNE dimensionality reduction.\n",
        "\n",
        "    :param embeddings: word embedding matrix of shape (words, dim). In the case\n",
        "                       of a word-word co-occurrence matrix the matrix shape would\n",
        "                       be (words, words).\n",
        "\n",
        "    :return\n",
        "        - 2-dimensional word embedding matrix of shape (words, 2)\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Running t-SNE reduction method... (it may take a while...)\")\n",
        "    tsne = TSNE(n_components=2, random_state=42, n_iter=1000, metric='cosine', \n",
        "                n_jobs=-1)\n",
        "    reduced = tsne.fit_transform(embeddings)\n",
        "    print(\"t-SNE reduction completed!\")\n",
        "    print(reduced.shape)\n",
        "\n",
        "    return reduced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIN0Nz-SL7T9"
      },
      "source": [
        "#### Most co-occurring word pairs\n",
        "We print the top co-occurring pairs by selecting the biggest values of the\n",
        "co-occurrence matrix.  \n",
        "\n",
        "Visualization is compared for window sizes of 1, 2 and 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drxDqGC_3SAa"
      },
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def topK_pairs(co_occurrence_matrix, k=10):\n",
        "    A = deepcopy(co_occurrence_matrix) #deepcopy is needed for sparse matrices\n",
        "\n",
        "    print(\"Top\",k,\"co-occurring word pairs\\n\")\n",
        "    for i in range(k):\n",
        "        index = np.argmax(A)\n",
        "        i = int(index/A.shape[0])\n",
        "        j = index%A.shape[0]\n",
        "        print(idx_to_word[i], idx_to_word[j], idx_to_word[i])\n",
        "        A[i, j], A[j, i] = 0, 0\n",
        "        \n",
        "    del A\n",
        "    gc.collect();\n",
        "    print(\"Pairs are printed 1-2-1 to underline the sequential nature in some of them:\")\n",
        "    print('\"plot twist\" vs \"twists plot\", \"york new\" vs \"new york\"\\n\\n')\n",
        "\n",
        "print(\"window size = 4\")\n",
        "topK_pairs(co_occurrence_matrix, k=10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhHqKS9hq3fn"
      },
      "source": [
        "A window size of 1 highlights very specific word pairs like \"new york\" while larger windows just show the most common words combined: \"one movie\", \"movie like\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3nTR0whPMAp"
      },
      "source": [
        "### t-SNE visualization with varying window size\n",
        "\n",
        "We now inquiry if adjectives and nouns can be respectively clustered in clear distinct groups by the method, with different window sizes for the co-occurrence matrix. For this purpose a small modification of the visualization method is performed, this to allow the insertion 2 different word groups to annotate, to be colored differently.\n",
        "\n",
        "  \n",
        "\n",
        "By varying the window size parameter a clear shift in the plots is seen: multiple clusters are visible for the smallest value, while they tend to spread out and vanish the more the window size grows.  \n",
        "This is predictable as a small window highlights common specific pairs like \"plot twist\", \"new york\", special effects\" and \"kung fu\" etc.  \n",
        "  \n",
        "When switching to a representative dataset of 10k samples and 50k words, this behavior clearly diminish, with window size of 4 still creating clearly visible clusters.\n",
        "\n",
        "We will compare these results with the visualization of the dense embeddings, anticipating a much more clear picture there.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wyeqb0TeGmbS"
      },
      "source": [
        "\n",
        "import textwrap # formatted printing\n",
        "\n",
        "common_adjectives = ['good', 'new', 'first', 'last', 'long', 'great', 'little', \n",
        "                     'own', 'other','old', 'right', 'big', 'high', 'different', \n",
        "                     'small', 'large', 'next', 'early', 'young', 'important', \n",
        "                     'few', 'public', 'bad', 'same', 'able','funny']\n",
        "\n",
        "common_nouns = ['horror', 'thriller', 'comedy','actor','time', 'person', 'year', \n",
        "                'way', 'day', 'thing', 'man', 'world',\n",
        "                'life', 'hand', 'part', 'child', 'eye', 'woman', 'place', 'work',\n",
        "                'week', 'case', 'point', 'government', 'company', 'number', 'group',\n",
        "                'problem', 'fact']\n",
        "\n",
        "for w in common_adjectives[:]:\n",
        "  if w not in word_to_idx.keys():\n",
        "    common_adjectives.remove(w)\n",
        "\n",
        "for w in common_nouns[:]:\n",
        "  if w not in word_to_idx.keys():\n",
        "    common_nouns.remove(w)\n",
        "\n",
        "print(\"Adjectives:\")\n",
        "print(textwrap.fill(' '.join(common_adjectives), 80))\n",
        "print(\"Nouns:\")\n",
        "print(textwrap.fill(' '.join(common_nouns), 80))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89auObOw_MM2"
      },
      "source": [
        "# t-SNE\n",
        "\n",
        "reduced_tSNE = reduce_tSNE(co_occurrence_matrix)\n",
        "visualize_embeddings(reduced_tSNE, common_nouns, common_adjectives, word_to_idx)\n",
        "\n",
        "print(\"window size =\",window_size)\n",
        "print(\"Nouns are in red, Adjectives are in yellow\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgEAAh1dOsXd"
      },
      "source": [
        "# SVD\n",
        "reduced_SVD = reduce_SVD(co_occurrence_matrix)\n",
        "visualize_embeddings(reduced_SVD, common_nouns, common_adjectives, word_to_idx)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxPTZE_U4qUk"
      },
      "source": [
        "### Embedding properties\n",
        "\n",
        "Visualization can give us a rough idea of how word embeddings are organized and if some semantic properties are reflected in the numerical dimensional space. For example, are synonyms close together? Ideally, if the dataset is big enough, we should see similar vector embeddings since synonyms usually have similar contexts.\n",
        "\n",
        "**How to do that?** We could highlight target words in the visualization step and check if our expectations are met. For instance, synonyms should be close together. However, this method is rather inaccurate and time-consuming (dimensionality reduction is not a perfect mapping). Thus, we need some sort of similarity metric that is independent of the vector dimensionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrhbF-lEESyW"
      },
      "source": [
        "#### **Cosine Similarity**\n",
        "\n",
        "Let us now consider again the matrix obtained using <code>co_occurrence_count</code>. \n",
        "Since we want to meaure how two word vectors are far apart, a naive solution would involve computing the dot product of the two vectors. However, this metric will give higher similarity either to longer vectors or to vectors that have higher counts.\n",
        "\n",
        "A better metric is **cosine similarity** which is just a normalized dot product.\n",
        "\n",
        "$s(p, q) = \\frac{p \\, \\cdot \\, q}{||p|| \\, \\cdot \\, ||q||}$\n",
        "\n",
        "where $s(p, q) \\in [-1, 1] $, since it computes the cosine of the angle between the two vectors. Intuitively, we are bringing vectors down to the d-dimensional unit sphere (d is the vocab size) and then computing their distance (in 2D space we will have a circle).\n",
        "\n",
        "Now, write down the cosine similarity formula so that we can proceed testing word embedding properties!\n",
        "\n",
        "**NOTE**: Since we are working with matrices, we will ask you to define a cosine similarity function that works with matrices. Extending to matrices is quite easy if you think them as lists of vectors.\n",
        "\n",
        "**NOTE**: It is permitted to use functions of existing packages (e.g. sci-kit learn). This is mainly for efficiency motivation. We are not going to discriminate between such solutions and the ones that manually implement the cosine similarity metric (since it is not the main objective of the assignment).\n",
        "\n",
        "**WHAT YOU HAVE TO DO**: First of all, try to manually define the cosine similarity operation. If your implementation is correct but not so efficient, you can define a separate cosine similarity function that leverages implementations of existing packages like sci-kit learn. Here we want to verify if you understand the metric, but at the same time we don't want to penalize you if you are not as efficient as possible (although it is one factor that it is important to not forget when coding in general)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr9y31Ft0_xm"
      },
      "source": [
        "### Cosine Similarity Method\n",
        "\n",
        "We provide the cosine similarity method in two versions.  \n",
        "We first hand-craft a solution that works well with dense matrices.  \n",
        "We rely on the scikit-learn provided cosine_similarity method for memory efficient computation with sparse matrices.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MvD51_joDD7"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity as cs_sklearn\n",
        "# Function definition\n",
        "\n",
        "def cosine_similarity(p, q, transpose_p=False, transpose_q=False):\n",
        "    \"\"\"\n",
        "    Computes the cosine similarity of two d-dimensional matrices\n",
        "\n",
        "    :param p: d-dimensional vector (np.ndarray) of shape (p_samples, d)\n",
        "    :param q: d-dimensional vector (np.ndarray) of shape (q_samples, d)\n",
        "    :param transpose_p: whether to transpose p or not\n",
        "    :param transpose_q: whether to transpose q or not\n",
        "\n",
        "    :return\n",
        "        - cosine similarity matrix S of shape (p_samples, q_samples)\n",
        "          where S[i, j] = s(p[i], q[j])\n",
        "    \"\"\"\n",
        "    \n",
        "    # If it is a vector, consider it as a single sample matrix\n",
        "    if len(p.shape) == 1:\n",
        "        p = p.reshape(1, -1)\n",
        "    if len(q.shape) == 1:\n",
        "        q = q.reshape(1, -1)\n",
        "        \n",
        "    dense_out = not hasattr(scipy.sparse, type(p).__name__)\n",
        "    return cs_sklearn(p, q, dense_output=dense_out)\n",
        "\n",
        "def cosine_similarity_handcraft(p, q, transpose_p=False, transpose_q=False):\n",
        "    # If it is a vector, consider it as a single sample matrix\n",
        "    if len(p.shape) == 1:\n",
        "        p = p.reshape(1, -1)\n",
        "    if len(q.shape) == 1:\n",
        "        q = q.reshape(1, -1)\n",
        "        \n",
        "    if hasattr(scipy.sparse, type(p).__name__):\n",
        "        print(\"Detected sparse co-occurrence matrix!\")\n",
        "        print(\"Converting to dense for hand crafted cosine similarity\")\n",
        "        p = p.todense()\n",
        "        q = q.todense()\n",
        "        return scipy.sparse.csr_matrix(np.dot(p , q.T) / np.outer(np.linalg.norm(p,axis=1) , np.linalg.norm(q,axis=1)))\n",
        "\n",
        "    return np.dot(p , q.T) / np.outer(np.linalg.norm(p,axis=1) , np.linalg.norm(q,axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCmDxRMtup6i"
      },
      "source": [
        "To test our hand-crafted method just call `cosine_similarity_handcraft()`\n",
        "\n",
        "Symmetry cheking on a large matrix (50k x 50x words with win_size = 4) consumes all available ram (25GB)even if sparse. This is due to the creation of co-occ.transpose().    \n",
        "Please test methods with the following smaller matrix to be sure to not crash the runtime or personal PC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "himvpltQuwCL"
      },
      "source": [
        "co_occurrence_bak = co_occurrence_matrix # backs up large co-occ matrix\n",
        "random_indexes = np.random.choice(np.arange(df.shape[0]),\n",
        "                                  size=500,\n",
        "                                  replace=False)\n",
        "\n",
        "df_small = df.iloc[random_indexes]\n",
        "\n",
        "co_occurrence_matrix = co_occurrence_count(df_small,idx_to_word, word_to_idx, window_size=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wufaiAs78E1E"
      },
      "source": [
        "# Testing\n",
        "\n",
        "print(\"Computing similarity matrix...\")\n",
        "similarity_matrix = cosine_similarity(co_occurrence_matrix,\n",
        "                                      co_occurrence_matrix,\n",
        "                                      transpose_q=True)\n",
        "print(\"Similarity completed!\")\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "def sparse_allclose(a, b, rtol=0, atol = 1e-8):\n",
        "    c = np.abs(np.abs(a - b) - rtol * np.abs(b))\n",
        "    return c.max() <= atol\n",
        "\n",
        "def evaluate_cosine_similarity(similarity_matrix):\n",
        "\n",
        "    # Vector similarity\n",
        "    print('[Cosine similarity Evaluation] Vector similarity check...')\n",
        "\n",
        "    p = np.array([5., 6., 0.3, 1.])\n",
        "    q = np.array([50., 6., 0., 0.])\n",
        "    assert np.allclose([[0.72074324]], cosine_similarity(p, q, transpose_q=True))\n",
        "\n",
        "    # Matrix similarity\n",
        "    print('[Cosine similarity Evaluation] Matrix similarity check...')\n",
        "\n",
        "    toy_matrix = np.array([5., 6., 0.3, 1.,\n",
        "                           50., 6., 0., 0.,\n",
        "                           0., 100., 20., 4.]).reshape(3, 4)\n",
        "    true_matrix = np.array([1., 0.72074324, 0.75852259,\n",
        "                            0.72074324, 1., 0.11674173,\n",
        "                            0.75852259, 0.11674173, 1.]).reshape(3, 3)\n",
        "    proposed_matrix = cosine_similarity(toy_matrix, toy_matrix, transpose_q=True)\n",
        "    print(proposed_matrix)\n",
        "    print(true_matrix)\n",
        "    assert np.allclose(proposed_matrix, true_matrix)\n",
        "\n",
        "    # There might be some numerical error that invalidates the np.equal check\n",
        "    assert np.allclose(proposed_matrix, proposed_matrix.transpose())\n",
        "\n",
        "    # Check symmetry\n",
        "    print(\"[Cosine similarity Evaluation] Symmetry checking...\")\n",
        "\n",
        "    is_sparse = False\n",
        "\n",
        "    if hasattr(scipy.sparse, type(similarity_matrix).__name__):\n",
        "        print(\"Detected sparse cosine similarity matrix!\")\n",
        "        is_sparse = True\n",
        "\n",
        "    if is_sparse:\n",
        "        try:\n",
        "            assert (similarity_matrix != similarity_matrix.transpose()).nnz == 0\n",
        "        except AssertionError:\n",
        "            assert sparse_allclose(similarity_matrix, similarity_matrix.transpose())\n",
        "    else:\n",
        "        # There might be some numerical error that invalidates the np.equal check\n",
        "        assert np.allclose(similarity_matrix, similarity_matrix.transpose())\n",
        "\n",
        "print('Evaluating cosine similarity...')\n",
        "evaluate_cosine_similarity(similarity_matrix)\n",
        "print('Evaluation completed!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lmk_jzNUtW9s"
      },
      "source": [
        "# retrieve backed up co_occ matrix and\n",
        "# compute similarity matrix\n",
        "if 'co_occurrence_bak' in globals():\n",
        "    co_occurrence_matrix = co_occurrence_bak\n",
        "\n",
        "similarity_matrix = cosine_similarity(co_occurrence_matrix, co_occurrence_matrix)\n",
        "\n",
        "if 'co_occurrence_bak' in globals():\n",
        "    del co_occurrence_bak\n",
        "gc.collect();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPnB9NrHETC8"
      },
      "source": [
        "#### **[Let's play!] Synonyms and Antonyms**\n",
        "\n",
        "Look for some words and provide a possible explanation of achieved results according to cosine similarity metric (you can also refer to previous visualization step).\n",
        "\n",
        "* Synonym pair: (w1, w2) such that w1 and w2 are synonyms\n",
        "* Antonyms pair: (w1, w2) such that w1 and w2 are antonyms\n",
        "* Synonym-Antonym triplet: (w1, w2, w3) such that w1 and w2 are synonyms and w1 and w3 are antonyms\n",
        "\n",
        "You can also support your answer by checking word contexts of selected words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UvgENf2FsO5"
      },
      "source": [
        "# We inquiry on common words in movie reviews\n",
        "\n",
        "'unhappy', 'sorrowful','dejected','regretful','depressed','downcast',\n",
        "\n",
        "print(\"Synonyms:\")\n",
        "print(similarity_matrix[word_to_idx['film'],word_to_idx['movie']])\n",
        "print(similarity_matrix[word_to_idx['great'],word_to_idx['good']],'\\n')\n",
        "\n",
        "print(\"Antonyms:\")\n",
        "print(similarity_matrix[word_to_idx['great'],word_to_idx['terrible']])\n",
        "print(similarity_matrix[word_to_idx['nice'],word_to_idx['awful']])\n",
        "print(similarity_matrix[word_to_idx['funny'],word_to_idx['sad']],'\\n')\n",
        "\n",
        "print(\"synonim-antonym triplet:\")\n",
        "print(similarity_matrix[word_to_idx['great'],word_to_idx['good']])\n",
        "print(similarity_matrix[word_to_idx['great'],word_to_idx['terrible']],'\\n')\n",
        "\n",
        "print(\"Unrelated pairs:\")\n",
        "print(similarity_matrix[word_to_idx['tissue'],word_to_idx['home']])\n",
        "print(similarity_matrix[word_to_idx['awfully'],word_to_idx['glass']])\n",
        "print(similarity_matrix[word_to_idx['liquid'],word_to_idx['movie']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu38-5ZmFvMg"
      },
      "source": [
        "** **HOW DO YOU EXPLAIN THE RESULTS OBTAINED IN YOUR EXPERIMENTS? DISCUSS HERE** **\n",
        "\n",
        "\n",
        "We chose to explore word similarities on movie-related ones, this is done so to have proper a properly populated count matrix.  \n",
        "  \n",
        "Synonyms get the highes cosine similarity score by far. This is expected as they are used in similar contexts to express similar meanings.\n",
        "Nonetheless, antonym get a fairly high socre as well. This is explainable as they are used in similar contextes as their counterparts.  \n",
        "\n",
        "Seemingly unrelated words show a considerably lower score, with some approaching the minimum cosine value of zero.  \n",
        "\n",
        "The windows size choice influences what gets very high scores: a bigger window is able to capture semantic meaning.  \n",
        "\n",
        "The dataset slice size influences atonyms and unrelated words, as a more sensible count matrix is constructed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM5e-2kXF4XI"
      },
      "source": [
        "#### **[Let's play!] Analogies**\n",
        "\n",
        "Another useful property to check is analogy resolution via word vectors. In particular, we might want to check if analogies such \"man : king == woman : x\" bring results like \"x = queen\".\n",
        "\n",
        "In order to do so, we first need to define a ranking function that returns the top $K$ most similar words of a given word vector. We might not want to be too much restrctive and play with $K \\ge 1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnoQdP-lGkzK"
      },
      "source": [
        "def get_top_K_indexes(data, K):\n",
        "    \"\"\"\n",
        "    Returns the top K indexes of a 1-dimensional array (descending order)\n",
        "    Example:\n",
        "        data = [0, 7, 2, 1]\n",
        "        best_indexes:\n",
        "        K = 1 -> [1] (data[1] = 7)\n",
        "        K = 2 -> [1, 2]\n",
        "        K = 3 -> [1, 2, 3]\n",
        "        K = 4 -> [1, 2, 3, 4]\n",
        "\n",
        "    :param data: 1-d dimensional array\n",
        "    :param K: number of highest value elements to consider\n",
        "\n",
        "    :return\n",
        "        - array of indexes corresponding to elements of highest value\n",
        "    \"\"\"\n",
        "    best_indexes = np.argsort(data, axis=0)[::-1]\n",
        "    best_indexes = best_indexes[:K]\n",
        "\n",
        "    return best_indexes\n",
        "\n",
        "def get_top_K_word_ranking(embedding_matrix, idx_to_word, word_to_idx,\n",
        "                           positive_listing, negative_listing, K):\n",
        "    \"\"\"\n",
        "    Finds the top K most similar words following this reasoning:\n",
        "        1. words that have highest similarity to words in positive_listing\n",
        "        2. words that have highest distance to words in negative_listing\n",
        "    \n",
        "    Positive and negative listing can be defined accordingly to a given analogy\n",
        "    Example:\n",
        "        \n",
        "        man : king :: woman : x\n",
        "    \n",
        "    positive_listing = ['king', 'woman']\n",
        "    negative_listing = ['man']\n",
        "\n",
        "    This is equivalent to: compute king - man + woman, and then find the\n",
        "    most similar candidate.\n",
        "    \n",
        "    :param embedding_matrix: embedding matrix of shape (words, embedding dimension).\n",
        "    Note that in the case of a co-occurrence matrix, the shape is (words, words).\n",
        "    :param idx_to_word: vocabulary map (index -> word) (dict)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param positive_listing: list of words that should have high similarity with\n",
        "                             top K retrieved ones.\n",
        "    :param negative_listing: list of words that should have high distance to\n",
        "                             top K retrieved ones.\n",
        "    :param K: number of best word matches to consider\n",
        "\n",
        "    :return\n",
        "        - top K word matches according to aforementioned criterium\n",
        "        - similarity values of top K word matches according to aforementioned\n",
        "          criterium\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Positive words (similarity)\n",
        "    positive_indexes = np.array([word_to_idx[word] for word in positive_listing])\n",
        "    word_positive_vector = np.sum(embedding_matrix[positive_indexes, :], axis=0)\n",
        "\n",
        "    # Negative words (distance)\n",
        "    negative_indexes = np.array([word_to_idx[word] for word in negative_listing])\n",
        "    word_negative_vector = np.sum(embedding_matrix[negative_indexes, :], axis=0)\n",
        "\n",
        "    # Find candidate words\n",
        "    target_vector = (word_positive_vector - word_negative_vector) / (len(positive_listing) + len(negative_listing))\n",
        "    total_indexes = np.concatenate((positive_indexes, negative_indexes))\n",
        "    valid_indexes = np.setdiff1d(np.arange(similarity_matrix.shape[0]), total_indexes)\n",
        "    candidate_vectors = embedding_matrix[valid_indexes]\n",
        "\n",
        "    candidate_similarities = cosine_similarity(candidate_vectors, target_vector, transpose_q=True)\n",
        "    candidate_similarities = candidate_similarities.ravel()\n",
        "\n",
        "    relative_indexes = get_top_K_indexes(candidate_similarities, K)\n",
        "    top_K_indexes = valid_indexes[relative_indexes]\n",
        "    top_K_words = [idx_to_word[idx] for idx in top_K_indexes]\n",
        "\n",
        "    return top_K_words, candidate_similarities[relative_indexes]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V4c-mOpNem7"
      },
      "source": [
        "Now do it yourself! Find some examples of analogies that hold and other that do not. Remember to give a proper explanation concerning obtained results.\n",
        "\n",
        "**Note**: 1-2 examples are sufficient. This exercies is just another way to inspect word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vva-_bylNxIE"
      },
      "source": [
        "### MODIFY THIS ###\n",
        "K = 20\n",
        "\n",
        "# Example analogy: tv : episodes :: film : x\n",
        "# positive listing -> [episodes, film]\n",
        "# negative listing ->  [tv]\n",
        "# masterpiece : superb :: x : tragic\n",
        "top_K_words, top_K_values = get_top_K_word_ranking(co_occurrence_matrix,\n",
        "                                                   idx_to_word,\n",
        "                                                   word_to_idx,\n",
        "                                                   ['actor', 'girl'],\n",
        "                                                   ['man'],\n",
        "                                                   K)\n",
        "print('Top K words: ', top_K_words)\n",
        "print('Top K values: ', top_K_values)\n",
        "\n",
        "top_K_words, top_K_values = get_top_K_word_ranking(co_occurrence_matrix,\n",
        "                                                   idx_to_word,\n",
        "                                                   word_to_idx,\n",
        "                                                   ['movie', 'many'],\n",
        "                                                   ['one'],\n",
        "                                                   K)\n",
        "\n",
        "print('Top K words: ', top_K_words)\n",
        "print('Top K values: ', top_K_values)\n",
        "\n",
        "top_K_words, top_K_values = get_top_K_word_ranking(co_occurrence_matrix,\n",
        "                                                   idx_to_word,\n",
        "                                                   word_to_idx,\n",
        "                                                   ['good', 'bad'],\n",
        "                                                   ['great'],\n",
        "                                                   K)\n",
        "print('Top K words: ', top_K_words)\n",
        "print('Top K values: ', top_K_values)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2jJPJHbN3hR"
      },
      "source": [
        "** **HOW DO YOU EXPLAIN THE RESULTS OBTAINED IN YOUR EXPERIMENTS? DISCUSS HERE** **\n",
        "\n",
        "It is necessary to consider a big sample of the dataset to obtain decent analogy results.  \n",
        "\n",
        "Top k terms show consistence of context with the positive word lists.  \n",
        "\n",
        "Sex of nouns can be retrieved: man : actor :: girl : actress.  \n",
        "\n",
        "Plural works, and even superlative relation: good : great :: bad : terrible !!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxGPC1CC7mYn"
      },
      "source": [
        "#### **[Let's play!] Bias**\n",
        "\n",
        "When we talk about societies we are usually aware of their related biases (gender, race, sexual orientation, etc..). Indeed, this fact is reflected at textual level. For example, when we consider the word 'doctor' we usually think of a 'man', whereas when we consider the word 'nurse' we usually think of a 'woman'.\n",
        "\n",
        "Let's see if word embeddings reflect such harmful biases. Find an example of bias by following the analogy approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkuKPZsJUPgH"
      },
      "source": [
        "top_K_words, top_K_values = get_top_K_word_ranking(co_occurrence_matrix,\n",
        "                                                   idx_to_word,\n",
        "                                                   word_to_idx,\n",
        "                                                   ['actor', 'girl'],\n",
        "                                                   ['guy'],\n",
        "                                                   K)\n",
        "print(\"\")\n",
        "print('Top K words: ', top_K_words)\n",
        "print('Top K values: ', top_K_values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kux9a29X1c9m"
      },
      "source": [
        "top_K_words, top_K_values = get_top_K_word_ranking(co_occurrence_matrix,\n",
        "                                                   idx_to_word,\n",
        "                                                   word_to_idx,\n",
        "                                                   ['native', 'white'],\n",
        "                                                   ['bad'],\n",
        "                                                   K)\n",
        "print('Top K words: ', top_K_words)\n",
        "print('Top K values: ', top_K_values)\n",
        "\n",
        "top_K_words, top_K_values = get_top_K_word_ranking(co_occurrence_matrix,\n",
        "                                                   idx_to_word,\n",
        "                                                   word_to_idx,\n",
        "                                                   ['actor', 'white'],\n",
        "                                                   ['native'],\n",
        "                                                   K)\n",
        "print('Top K words: ', top_K_words)\n",
        "print('Top K values: ', top_K_values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z45pDkq_1iX0"
      },
      "source": [
        "top_K_words, top_K_values = get_top_K_word_ranking(co_occurrence_matrix,\n",
        "                                                   idx_to_word,\n",
        "                                                   word_to_idx,\n",
        "                                                   ['actor', 'gay'],\n",
        "                                                   ['hetero'],\n",
        "                                                   K)\n",
        "\n",
        "print('Top K words: ', top_K_words)\n",
        "print('Top K values: ', top_K_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twScPzVqUSmt"
      },
      "source": [
        "** **HOW DO YOU EXPLAIN THE RESULTS OBTAINED IN YOUR EXPERIMENTS? DISCUSS HERE** **\n",
        "\n",
        "We look for examples of possible biases for gender, ethnical identity and sexual orienting.  \n",
        "This is done by subtracting a significative term and adding its counterpart, looking for biased top K results.  \n",
        "We remain in the cinema semantic area, expecting better results in this more represented field.  \n",
        "\n",
        "Drawing conclusions is not easy at all form this dataset and embedding!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6cyDMwtLWQ4"
      },
      "source": [
        "## Better sparse word embeddings\n",
        "\n",
        "Until now we've played with the most basic type of word encoding, that is count-based co-occurrence matrix. However, there are better ways to encode words.\n",
        "\n",
        "In particular, we will explore positive pointwise mutual information (PPMI) weighting technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvsS4nY6LjOm"
      },
      "source": [
        "### PPMI\n",
        "\n",
        "Pointwise mutual information (PMI) is a weighting technique, just like tf-dif, that gives more weight to word pairs based on how often they occur within the same context window that we would have expected them to appear by chance.\n",
        "\n",
        "$PMI(w1, w2) = \\log_2 \\frac{P(w1, w2)}{P(w1)P(w2)}$\n",
        "\n",
        "PMI value range is $[-\\infty, \\infty]$, but negative values are a bit tricky, unless we have a very big corpus. Thus, it is more common to replace all negative PMI values with zero.\n",
        "\n",
        "$PPMI(w1, w2) = \\max(\\log_2 \\frac{P(w1, w2)}{P(w1)P(w2)}, 0)$\n",
        "\n",
        "Now, it's your turn to weight the count-based co-occurrence matrix with PPMI technique.\n",
        "\n",
        "**PPMI Memo**: \n",
        "Given a co-occurrence matrix C of shape (N, M), we can turn it into a PPMI matrix as follows:\n",
        "\n",
        "$p_{i,j} = \\frac{C_{i, j}}{\\sum_{i=1}^N \\sum_{j=1}^M C_{i,j}}$\n",
        "\n",
        "$p_{i,*} = \\frac{\\sum_{j=1}^M C_{i, j}}{\\sum_{i=1}^N \\sum_{j=1}^M C_{i,j}}$\n",
        "\n",
        "$p_{*,j} = \\frac{\\sum_{i=1}^N C_{i, j}}{\\sum_{i=1}^N \\sum_{j=1}^M C_{i,j}}$\n",
        "\n",
        "$PPMI_{i, j} = \\max(\\log_2 \\frac{p_{i, j}}{p_{i, *} \\, p_{*, j}}, 0)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vouLzKbM62Td"
      },
      "source": [
        "### PPMI implementation\n",
        "PPMI computation on dense matrices is fast \"numpy implementation blabla\"\n",
        "\n",
        "we talked about sparse discussed bla bla\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOZM3hQjE0rA"
      },
      "source": [
        "co_occurrence_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DcQJ1dvmCY2"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "# Function definition\n",
        "def convert_ppmi(co_occurrence_matrix):\n",
        "    \"\"\"\n",
        "    Converts a count-based co-occurrence matrix to a PPMI matrix\n",
        "    :param co_occurrence_matrix: count based co-occurrence matrix of shape (|V|, |V|)\n",
        "    :return\n",
        "        - PPMI co-occurrence matrix of shape (|V|, |V|)\n",
        "    \"\"\"\n",
        "    den = np.sum(co_occurrence_matrix)\n",
        "\n",
        "    if hasattr(scipy.sparse, type(co_occurrence_matrix).__name__):\n",
        "        norm_x = preprocessing.normalize(co_occurrence_matrix, norm='l1', axis=0)# fast on sparse\n",
        "        ppmi = preprocessing.normalize(co_occurrence_matrix, norm='l1', axis=1)\n",
        "\n",
        "        ppmi_mask = ppmi.nonzero()\n",
        "        ppmi[ppmi_mask] = ppmi[ppmi_mask] / co_occurrence_matrix[ppmi_mask]\n",
        "        ppmi = ppmi.multiply(norm_x)\n",
        "        ppmi = ppmi.multiply(den)\n",
        "        ppmi.data = np.log2(ppmi.data)\n",
        "        ppmi[ppmi < 0] = 0\n",
        "\n",
        "        return ppmi\n",
        "\n",
        "    pij = np.sum(co_occurrence_matrix, axis=0)\n",
        "    ppmi = np.log2(co_occurrence_matrix / np.outer(pij, pij)) + np.log2(den)\n",
        "    ppmi[ppmi<0] = 0\n",
        "\n",
        "\n",
        "    return ppmi\n",
        "\n",
        "# Testing\n",
        "\n",
        "def sparse_allclose(a, b, rtol=0, atol = 1e-8):\n",
        "    c = np.abs(np.abs(a - b) - rtol * np.abs(b))\n",
        "    return c.max() <= atol\n",
        "\n",
        "print(\"Computing PPMI co-occurrence matrix...\")\n",
        "ppmi_occurrence_matrix = convert_ppmi(co_occurrence_matrix)\n",
        "print(\"PPMI completed!\")\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "def evaluate_ppmi_matrix(matrix):\n",
        "    is_sparse = False\n",
        "\n",
        "    if hasattr(scipy.sparse, type(matrix).__name__):\n",
        "        print(\"Detected sparse PPMI co-occurrence matrix!\")\n",
        "        is_sparse = True\n",
        "\n",
        "    # Check symmetry\n",
        "    print(\"[Co-occurrence PPMI matrix Evaluation] Symmetry checking...\")\n",
        "    if is_sparse:\n",
        "        try:\n",
        "            assert (matrix != matrix.transpose()).nnz == 0\n",
        "        except AssertionError:\n",
        "            assert sparse_allclose(matrix, matrix.transpose())\n",
        "    else:\n",
        "        try:\n",
        "            assert np.equal(matrix, matrix.transpose()).all()\n",
        "        except AssertionError:\n",
        "            assert np.allclose(matrix, matrix.transpose())\n",
        "\n",
        "    # A very simple example\n",
        "    print(\"[Co-occurrence PPMI matrix Evaluation] Toy example checking...\")\n",
        "\n",
        "    toy_df = pd.DataFrame.from_dict({\n",
        "        'sentence_1': [\"All that glitters is not gold\"],\n",
        "        'sentence_2': [\"All in all I like this assignment\"]\n",
        "    })\n",
        "\n",
        "    # We should already have download co-occurrence benchmark data\n",
        "    benchmark_path = os.path.join(os.getcwd(), 'Benchmark')\n",
        "    toy_path = os.path.join(benchmark_path, 'co-occurrence_count_benchmark')\n",
        "    toy_idx_to_word = np.load(os.path.join(toy_path, 'toy_idx_to_word.npy'), allow_pickle=True).item()\n",
        "    toy_word_to_idx = np.load(os.path.join(toy_path, 'toy_word_to_idx.npy'), allow_pickle=True).item()\n",
        "    toy_valid_matrix = np.load(os.path.join(toy_path, 'toy_co_occurrence_matrix_count.npy'))\n",
        "\n",
        "    toy_ppmi_matrix = convert_ppmi(toy_valid_matrix)\n",
        "    toy_valid_ppmi_matrix = np.load(os.path.join(toy_path, 'toy_co_occurrence_matrix_ppmi.npy'))\n",
        "\n",
        "    if is_sparse:\n",
        "        assert sparse_allclose(toy_ppmi_matrix, toy_valid_ppmi_matrix)\n",
        "    else:\n",
        "        try:\n",
        "            assert np.equal(toy_ppmi_matrix, toy_valid_ppmi_matrix).all()\n",
        "        except AssertionError:\n",
        "            assert np.allclose(toy_ppmi_matrix, toy_valid_ppmi_matrix)\n",
        "\n",
        "\n",
        "print('Evaluating PPMi matrix conversion...')\n",
        "evaluate_ppmi_matrix(ppmi_occurrence_matrix)\n",
        "print('Evaluation completed!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vg7bGQs8JFn"
      },
      "source": [
        "### PPMI with dense matrices CUDA implementation\n",
        "\n",
        "We implement a fast dense PPMI computation method to run on GPU.  \n",
        "\n",
        "Dense test matrix must be provided for testing, we recommend using a small matrix to dont overflow available ram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y6An7LQZJz2",
        "cellView": "both"
      },
      "source": [
        "!find / -iname 'libdevice'\n",
        "!find / -iname 'libnvvm.so'\n",
        "\n",
        "from __future__ import division\n",
        "from numba import cuda\n",
        "import numpy\n",
        "import math\n",
        "import os\n",
        "\n",
        "os.environ['NUMBAPRO_LIBDEVICE'] = \"/usr/local/cuda-10.0/nvvm/libdevice\"\n",
        "os.environ['NUMBAPRO_NVVM'] = \"/usr/local/cuda-10.0/nvvm/lib64/libnvvm.so\"\n",
        "\n",
        "def convert_ppmi_cuda(co_occurrence_matrix):\n",
        "\n",
        "    if hasattr(scipy.sparse, type(co_occurrence_matrix).__name__):\n",
        "        print(\"Dense matrix required for CUDA implementation\")\n",
        "        return None\n",
        "\n",
        "    bound = co_occurrence_matrix.shape[0]\n",
        "    den = np.sum(co_occurrence_matrix)\n",
        "    pij = np.sum(co_occurrence_matrix, axis=0) \n",
        "    \n",
        "    # CUDA kernel\n",
        "    @cuda.jit\n",
        "    def my_kernel(gdata, gpij, gppmi):\n",
        "        i, j = cuda.grid(2)\n",
        "        gppmi[i, j] = gdata[i,j]*den/(gpij[i]*gpij[j])\n",
        "\n",
        "    # Host code   \n",
        "    # Copy the arrays to the device\n",
        "    g_ccm = cuda.to_device(co_occurrence_matrix)\n",
        "    g_pij = cuda.to_device(pij)\n",
        "\n",
        "    # Allocate memory on the device for the result\n",
        "    g_ppmi = cuda.device_array((bound, bound))\n",
        "\n",
        "    # Configure the blocks\n",
        "    threadsperblock = (16, 16)\n",
        "    blockspergrid_x = int(math.ceil(g_ccm.shape[0] / threadsperblock[0]))\n",
        "    blockspergrid_y = int(math.ceil(g_ccm.shape[1] / threadsperblock[1]))\n",
        "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
        "\n",
        "    # Start the kernel \n",
        "    my_kernel[blockspergrid, threadsperblock](g_ccm, g_pij, g_ppmi)\n",
        "\n",
        "    # Copy the result back to the host\n",
        "    ppmi = g_ppmi.copy_to_host()\n",
        "\n",
        "    ppmi = np.log2(ppmi)\n",
        "    ppmi[ppmi<0] = 0\n",
        "    return ppmi\n",
        "\n",
        "ppmi_occurrence_matrix= convert_ppmi_cuda(co_occurrence_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQOx7ZtLyJTn"
      },
      "source": [
        "### Visualization (cont'd)\n",
        "\n",
        "Let's see if these weighting techniques have brought some change at visualization level!\n",
        "\n",
        "Pick a dimensionality reduction technique and explore the new embedding space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa2E6yWDyb39"
      },
      "source": [
        "# t-SNE\n",
        "\n",
        "reduced_tSNE = reduce_tSNE(ppmi_occurrence_matrix)\n",
        "visualize_embeddings(reduced_tSNE, common_nouns, common_adjectives, word_to_idx)\n",
        "\n",
        "print(\"window size =\",window_size)\n",
        "print(\"Nouns are in red, Adjectives are in yellow\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7Ud--aA8Jlw"
      },
      "source": [
        "Feel free to play with visualization!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY70qU-53rFy"
      },
      "source": [
        "### [Let's play!] Embedding properties (cont'd)\n",
        "\n",
        "Choose a word embedding property to analyse (synonyms, analogies, bias) and either select and describe a new example or pick an old one and compare previously achieved results with current ones!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGvEcMrs4Yvq"
      },
      "source": [
        "print(\"Computing similarity matrix...\")\n",
        "ppmi_similarity_matrix = cosine_similarity(ppmi_occurrence_matrix,\n",
        "                                      ppmi_occurrence_matrix,\n",
        "                                      transpose_q=True)\n",
        "\n",
        "# We inquiry on common words in movie reviews\n",
        "\n",
        "print(\"Synonyms:\")\n",
        "print(ppmi_similarity_matrix[word_to_idx['film'],word_to_idx['movie']])\n",
        "print(ppmi_similarity_matrix[word_to_idx['great'],word_to_idx['good']],'\\n')\n",
        "\n",
        "print(\"Antonyms:\")\n",
        "print(ppmi_similarity_matrix[word_to_idx['great'],word_to_idx['terrible']])\n",
        "print(ppmi_similarity_matrix[word_to_idx['nice'],word_to_idx['awful']])\n",
        "print(ppmi_similarity_matrix[word_to_idx['funny'],word_to_idx['sad']],'\\n')\n",
        "\n",
        "print(\"synonim-antonym triplet:\")\n",
        "print(ppmi_similarity_matrix[word_to_idx['great'],word_to_idx['good']])\n",
        "print(ppmi_similarity_matrix[word_to_idx['great'],word_to_idx['terrible']],'\\n')\n",
        "\n",
        "print(\"Unrelated pairs:\")\n",
        "print(ppmi_similarity_matrix[word_to_idx['tissue'],word_to_idx['home']])\n",
        "print(ppmi_similarity_matrix[word_to_idx['awfully'],word_to_idx['glass']])\n",
        "print(ppmi_similarity_matrix[word_to_idx['liquid'],word_to_idx['movie']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp9PjerwH5Cd"
      },
      "source": [
        "Synonyms, Antonyms and Synonyms-Antonyms triplet results with PPMI are shown.  \n",
        "\n",
        "Cosine similarity on the PPMI computed embedding show greatly lower values.  \n",
        "This could be due the clipping of negative values, but further tests are required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc4YevLsx3xY"
      },
      "source": [
        "# [Part II] Dense embeddings\n",
        "\n",
        "Until now we've worked with sparse embedding methods, which lead to high dimensional word embeddings (dimension equal to |V|). The main drawback of such approach is that words belong to separate dimensions. Thus, in order to check if two words have similar contexts we need to have a large corpus available.\n",
        "\n",
        "To this end, we might prefer a dense embedding technique, such that all words are encoded to high dimensional space, much smaller than |V| (generally up to $\\sim$ 1000). A dense representation is also convenient from a machine learning point of view: we have fewer parameters to learn and, thus, models are less prone to overfitting. Moreover, words do not belong to separate dimensions anymore and semantic relationships are easily modelled.\n",
        "\n",
        "In this section, we will experiment with pre-trained dense embedding models and compare them to previously described sparse methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i_gxgZK5VGM"
      },
      "source": [
        "## Working with a pre-trained model\n",
        "\n",
        "The first step consists in choosing and downloading a pre-trained embedding model. For the purpose of this assignment, we limit to classic models, such as Word2Vec and GloVe.\n",
        "\n",
        "Furthermore, some pre-trained embedding model versions may be quite resource demanding, depending on the embedding dimension and on the vocabulary size. We recommend sticking to low dimensional spaces (50, 100, 200) to avoid being stuck waiting for too much time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoU1fqAs5XxI"
      },
      "source": [
        "### Download embedding model\n",
        "\n",
        "Downloading a pre-trained embedding model is quite simple to due existing ad hoc wrappers. In particular, we will use [Gensim](https://radimrehurek.com/gensim/) library for both embedding models as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV0RQIFT-Sd3"
      },
      "source": [
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "\n",
        "def load_embedding_model(model_type, embedding_dimension=50):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained word embedding model via gensim library.\n",
        "\n",
        "    :param model_type: name of the word embedding model to load.\n",
        "    :param embedding_dimension: size of the embedding space to consider\n",
        "\n",
        "    :return\n",
        "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
        "    \"\"\"\n",
        "\n",
        "    download_path = \"\"\n",
        "\n",
        "    # Find the correct embedding model name\n",
        "    if model_type.strip().lower() == 'word2vec':\n",
        "        download_path = \"word2vec-google-news-300\"\n",
        "\n",
        "    elif model_type.strip().lower() == 'glove':\n",
        "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "\n",
        "    else:\n",
        "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove\")\n",
        "\n",
        "    # Check download\n",
        "    try:\n",
        "        emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "        print(\"Word2Vec: 300\")\n",
        "        print(\"Glove: 50, 100, 200, 300\")\n",
        "        raise e\n",
        "\n",
        "    return emb_model\n",
        "\n",
        "\n",
        "# Modify these variables as you wish!\n",
        "# Glove -> 50, 100, 200, 300\n",
        "# Word2Vec -> 300\n",
        "embedding_model_type = \"Glove\"\n",
        "embedding_dimension = 50\n",
        "\n",
        "embedding_model = load_embedding_model(embedding_model_type, embedding_dimension)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEL3Hrjl5dfs"
      },
      "source": [
        "### Out of vocabulary (OOV) words\n",
        "\n",
        "Before evaluating pre-trained dense word embeddings, it is good practice to check if the model is consistent with our dataset. To do so, we check the number of out-of-vocabulary (OOV) terms.\n",
        "\n",
        "If the OOV amount is negligible, we can just keep going. On the other hand, we might want to handle OOV terms by assigning them a specific word vector.\n",
        "\n",
        "**Which one?** One common practice is to assign a random vector, since the embedding model will be part of a deep learning model and, thus, word vectors might be trained during the learning process. Even if that is the case, we can assign an embedding that is more meaningful rather than a random one: for example, we can identify the word embedding of an OOV term as the mean of its neighbour word embeddings.\n",
        "\n",
        "Check out OOV terms and assign a meaningful word embedding. Then, at the visualization step, check if this strategy reflects words semantic properties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPrXS_dX9lvu"
      },
      "source": [
        "### OOV identification\n",
        "`word_listing` terms are found by checking against external `embedding_model` ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_L_ewVPH7VQ"
      },
      "source": [
        "# Function definition\n",
        "\n",
        "def check_OOV_terms(embedding_model, word_listing):\n",
        "    \"\"\"\n",
        "    Checks differences between pre-trained embedding model vocabulary\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_listing: dataset specific vocabulary (list)\n",
        "\n",
        "    :return\n",
        "        - list of OOV terms\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE ###\n",
        "    oov_count = 0\n",
        "    br_count = 0\n",
        "    oov_terms = []\n",
        "    for word in word_listing:\n",
        "        try:\n",
        "            embedding_model[word]\n",
        "        except KeyError:\n",
        "            oov_count +=1\n",
        "            oov_terms.append(word)\n",
        "            if word.endswith('br'):\n",
        "                br_count +=1\n",
        "    \n",
        "    return oov_terms\n",
        "\n",
        "oov_terms = check_OOV_terms(embedding_model, word_listing)\n",
        "\n",
        "print(\"Total OOV terms: {0} ({1:.2%})\".format(len(oov_terms), float(len(oov_terms)) / len(word_listing)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjNoxZb--5O-"
      },
      "source": [
        "### Handling OOV words\n",
        "\n",
        "Now we proceed on building the embedding matrix, while handling OOV terms at the same time. \n",
        "\n",
        "Experiment with/without the OOV custom encoding strategy.\n",
        "\n",
        "**NOTE**: Here we ask you to implement both OOV strategies! Feel free to either write two separate functions or modify the given function signature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC1F_44lJUDF"
      },
      "source": [
        "# Function definition\n",
        "\n",
        "def build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, oov_terms, oov_rnd=False):\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "    :param co_occorruence_count_matrix: the co-occurrence count matrix of the given dataset (window size 1)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    if oov_rnd:\n",
        "        rnd_vec = np.random.rand(len(oov_terms), embedding_dimension)\n",
        "\n",
        "\n",
        "    for i, oov in enumerate(oov_terms):\n",
        "        neighbours = similarity_matrix[word_to_idx[oov], :]\n",
        "        if hasattr(scipy.sparse, type(neighbours).__name__):\n",
        "            neighbours = np.array(neighbours.todense()).squeeze()\n",
        "            \n",
        "        neighbours = neighbours.argsort()[::-1][:10]\n",
        "        if oov_rnd:\n",
        "            embedding_model.add(oov, rnd_vec[i])\n",
        "            continue\n",
        "        mean = np.zeros((embedding_dimension,), dtype=np.float)\n",
        "        inc = 0\n",
        "        for n in neighbours:\n",
        "            if idx_to_word[n] in oov_terms:\n",
        "                continue\n",
        "            mean += embedding_model[idx_to_word[n]]\n",
        "            inc += 1\n",
        "        mean /= inc\n",
        "        embedding_model.add(oov, mean) \n",
        "    return embedding_model.vectors\n",
        "\n",
        "# Testing\n",
        "\n",
        "embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, oov_terms)\n",
        "\n",
        "print(\"Embedding matrix shape: {}\".format(embedding_matrix.shape))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hirKvw8x5kd4"
      },
      "source": [
        "## Embedding visualization (cont'd)\n",
        "\n",
        "We are now ready to visualize pre-trained word embeddings!  \n",
        "\n",
        "\n",
        "Dense embeddings are so commonly used, that their visualization is possible with Tensorboard, other than the provided methods.\n",
        "This has the great advantage of interactivity.\n",
        "\n",
        "To use it one should oper the  \"Projector\" tab on the top-right.  \n",
        "Then t-SNE can be selected from the bottom left, and one cas search for any word and visualize it's neighbors!  \n",
        "\n",
        "One can see how this embedding is very well trained by searching the following:  \n",
        "- year numbers are grouped: 1957, 2000, etc.\n",
        "- US states: California\n",
        "- and many more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82Fjuqg9SJIO"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter()\n",
        "keys = list(embedding_model.vocab.keys())\n",
        "writer.add_embedding(embedding_model.vectors[:5000], keys[:5000])\n",
        "writer.close()\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AApBQmfC8STB"
      },
      "source": [
        "Feel free to play with visualization!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxVC_fOo5oqQ"
      },
      "source": [
        "## [Let's play!] Embedding properties (cont'd)\n",
        "\n",
        "Choose a word embedding property to analyse (synonyms, analogies, bias) and either select and describe a new example or pick an old one and compare previously achieved results with current ones!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-diOYtpnzxIZ"
      },
      "source": [
        "king, man, woman = 'actor','man','men'\n",
        "#embedding_model.most_similar_to_given(embedding_model[king] - embedding_model[man] + embedding_model[woman])\n",
        "\n",
        "def get_top_analogies(king, man, woman):\n",
        "    queen_emb = embedding_model[king] - embedding_model[man] + embedding_model[woman]\n",
        "    most_similar =  embedding_model.similar_by_vector(queen_emb, topn=5)\n",
        "    print(f\"{man} : {king} :: {woman} : \")\n",
        "    print(*most_similar, sep='\\n')\n",
        "    print()\n",
        "    return(most_similar)\n",
        "    \n",
        "    \n",
        "print(\"Gender:\")\n",
        "get_top_analogies('actor','man','woman')\n",
        "get_top_analogies('doctor','woman','man')\n",
        "\n",
        "print(\"Plurals:\")\n",
        "get_top_analogies('actor','man','men')\n",
        "get_top_analogies('king','man','woman');\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyPDNB56UhmM"
      },
      "source": [
        "# Contact\n",
        "\n",
        "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "Don't forget that your feedback is very important! Your suggestions help us improving course material."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNXyqKePNbth"
      },
      "source": [
        "# FAQ\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Is it ok if I work with a small slice of the dataset?**\n",
        "\n",
        "**A:** Yes, it is perfectly ok! The aim of this assignment is to look at word embedding methods and assess semantic properties. Large datasets usually imply large vocabularies and efficient sparse encoding methods have to be considered. Since such methods (see scipy documentation) might be complex to handle (especially under a colab session), you are free to work with small corpora.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Do I have to use both dimensionality reduction methods?**\n",
        "\n",
        "**A:** Just one is fine! We suggest to try both of them at least once!\n",
        "\n",
        "---\n",
        "\n",
        "**Q: I'm struggling find good examples for analogies, bias and other scenarios!**\n",
        "\n",
        "**A:** It is perfectly fine, this is just a simple example where we want you to look at\n",
        "different encoding methods. Try to find at least one example that sounds good to you and motivate obtained results. Most probably, you won't find a perfect corrispondence of your expectations, but for us what's important is that you develop a critic approach, baring in mind that data has to explored before anything else.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Isn't stopwords removal excessive?**\n",
        "\n",
        "**A:** Indeed, removing all stopwords might alter sentence meaning! However, they also alter co-occurrence matrices due to their high frequency. Not ignoring them leads to poor results when considering semantic properties. It's up to you whether removing or keeping stopwords! (remember to comment the corresponding method under [Some Cleaning](https://colab.research.google.com/drive/1UkGz0vdhPXh9NeApG7mYtY6e-jRcR3if#scrollTo=2TLTu0-2JQwi&line=3&uniqifier=1) section.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Can we modify functions signature?**\n",
        "\n",
        "**A:** Functions that you have to complete can be modified as you please! Current functions definition should consider all required inputs in most cases.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Can we modify the dataset slicing step**\n",
        "\n",
        "**A:** Yes, of course! The current slicing is just one possibility.\n",
        "\n",
        "---\n"
      ]
    }
  ]
}